{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## create a tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfb367c654e4b96d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "path = \"../data/input.txt\"\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print first 1000 characters\n",
    "print(data[:1000])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d14bfdff032b90f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "unique_chars = sorted(list(set(data)))\n",
    "vocabulary_size = len(unique_chars)\n",
    "print(''.join(unique_chars))\n",
    "print(vocabulary_size)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcdb63b62721927b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tokenization of characters\n",
    "encoder_func = {ch: i for i, ch in enumerate(unique_chars)}\n",
    "decoder_func = {i: ch for i, ch in enumerate(unique_chars)}\n",
    "\n",
    "encoder = lambda s: [encoder_func[c] for c in s]\n",
    "decoder = lambda c: ''.join([decoder_func[i] for i in c])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a60dc3404e2549b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(encoder(\"hii there\"))\n",
    "print(decoder(encoder(\"hii there\")))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cc3a9a6a2557432",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tensor_data = torch.LongTensor(encoder(data))\n",
    "print(tensor_data.size())\n",
    "print(tensor_data.dtype)\n",
    "print(tensor_data[:1000])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3f5d9ee2fd2c2d6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_upper_index = int(0.9 * len(tensor_data))\n",
    "train_data, test_data = tensor_data[:train_upper_index], tensor_data[train_upper_index:]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "802ec66c533cd756",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "context_length = 8\n",
    "train_data[:context_length + 1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab8de7efd86aba95",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# this way we train the transformer to predict on context from size of 1 up until context_size\n",
    "\n",
    "x = train_data[:context_length]\n",
    "y = train_data[1:context_length + 1]\n",
    "for t in range(context_length):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context}, target is {target}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75f4b840e40da096",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "context_length = 8\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    idx = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i + context_length] for i in idx])\n",
    "    y = torch.stack([data[i + 1:i + context_length + 1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('****************')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(context_length):\n",
    "        context = xb[b, :t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()}, target is {target}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34a76f16968595eb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# implement a simple language model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from the lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (b, t) tensor of type int\n",
    "        logits = self.token_embedding_table(idx)  # (batch, time, channels)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            b, t, c = logits.shape\n",
    "            logits = logits.view(b * t, c)\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (b, t) array of indices in current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(idx)\n",
    "            # take only the last time step prediction\n",
    "            logits = logits[:, -1, :]\n",
    "            # calculate the probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)  # (b, t+1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramModel(vocab_size=vocabulary_size)\n",
    "out, loss = m(xb, yb)\n",
    "print(out.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decoder(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef86421f1a61bf7a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cbb7ac99b13439a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    # sample a batch\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # forward\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0314cb3caea6ad9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# still not shakespeare, but we're making progress \n",
    "print(decoder(m.generate(idx, max_new_tokens=500)[0].tolist()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e73561d29f443af",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## mathematical trick in self-attention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6a12e5004a227c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "b, t, c = 4, 8, 2  # batch, time, channels\n",
    "x = torch.randn(b, t, c)\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90c893eeee0ed8a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for each batch sequence, we want to calculate the average of each vector leading to each t_th token\n",
    "xbow = torch.zeros((b, t, c))\n",
    "for batch in range(b):\n",
    "    for time in range(t):\n",
    "        xprev = x[batch, :time + 1]  # (time,c)\n",
    "        xbow[batch, time] = torch.mean(xprev, dim=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3a226eff63a83e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xbow[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "131502823363088a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# making it very efficient using matrix multiplication\n",
    "torch.manual_seed(1305)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"c=\")\n",
    "print(c)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7edb347a20ff510"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The idea is to create an \"sum\" or \"average\" kernel using matrix a which is the \"operation\" matrix, and b which is the \"value\" matrix.\n",
    "# in the example below, a @ b will yield matrix c, which will consist the averages of the columns in b, up until the i_th row for each row i in matrix b.\n",
    "\n",
    "torch.manual_seed(1305)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"c=\")\n",
    "print(c)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6113283f8055de5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weight_matrix = torch.tril(torch.ones(t, t))\n",
    "weight_matrix = weight_matrix / weight_matrix.sum(1, keepdim=True)\n",
    "\n",
    "xbow2 = weight_matrix @ x  # (b, t, t) @ (b, t, c)\n",
    "xbow2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "768eb28c4ef28eba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.allclose(xbow, xbow2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20515d62cde8eccd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# another version using softmax\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "tril = torch.tril(torch.ones(t, t))\n",
    "weight_matrix = torch.zeros((t, t))\n",
    "weight_matrix = weight_matrix.masked_fill(tril == 0, float('-inf'))\n",
    "weight_matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89bce67eacdc9c93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weight_matrix = softmax(weight_matrix, dim=-1)\n",
    "weight_matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89c1c26fa5f8cd8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xbow3 = weight_matrix @ x\n",
    "torch.allclose(xbow, xbow3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a90ce8577e87cd4c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## implementation of self-attention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49af38e63bcc2dfe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.set_printoptions(linewidth=200)\n",
    "torch.manual_seed(1305)\n",
    "b, t, c = 4, 8, 32  # batch, time, channels\n",
    "x = torch.randn(b, t, c)\n",
    "\n",
    "tril = torch.tril(torch.ones(t, t))\n",
    "weight_matrix = torch.zeros((t,t))\n",
    "weight_matrix = weight_matrix.masked_fill(tril == 0, float('-inf'))\n",
    "weight_matrix = F.softmax(weight_matrix, dim=1)\n",
    "\n",
    "out = weight_matrix @ x\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3d6e3cf24e62091"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tril"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb20e338ca07cd0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weight_matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "208ddf7864eec717"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# so we don't want the affinities between the tokens to be uniform. I'll probably want to weight or select different data points from my past to be more or less significant to the current step, and to do so based on the data. This problem is solved by self-attention.\n",
    "\n",
    "# every single node (position) in the vector will emmit two vectors:\n",
    "# 1. the query vector (or q)\n",
    "# 2. the key vector (or k)\n",
    "# 3. the value vector (or v)\n",
    "\n",
    "# the query vector will roughly speaking encode \"what am I looking for\" and the key vector will encode \"what do I contain\"\n",
    "# the dot product between q and k will share information between nodes, for what each of the nodes is looking for, and what each of them contains.\n",
    "\n",
    "# later we introduce a value vector v. It will store a value for each node in x, sort of saying: \"q is what I'm looking for, k is what I have to offer, and if you find me interesting, v is what I will communicate with you.\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f6ca60fc780b268"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### implementation of a single head of self-attention "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "733e58d90879529f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "head_size = 16\n",
    "key = nn.Linear(c, head_size, bias=False)\n",
    "query = nn.Linear(c, head_size, bias=False)\n",
    "value = nn.Linear(c, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (b, t, 16)\n",
    "q = query(x) # (b, t, 16)\n",
    "\n",
    "\n",
    "weight_matrix = q @ k.transpose(-2, -1) * head_size**-0.5 # carful not to transpose the batch dim,  (b, t, 16) @ (b, t, 16) --> (b, t, t) which is the affinities matrix for each element in the batch\n",
    "# multiplying by the square root of the head size is important to ensure good initialization of variance\n",
    "\n",
    "tril = torch.tril(torch.ones(t, t))\n",
    "weight_matrix = weight_matrix.masked_fill(tril == 0, float('-inf'))\n",
    "weight_matrix = F.softmax(weight_matrix, dim=1)\n",
    "\n",
    "v = value(x)\n",
    "out = weight_matrix @ v"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8387b4bea31f5f5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weight_matrix[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4c3f4ebaa39334a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In Encoder block - we might want to allow all nodes to talk to each other (backward and forward in time), in the case of \"sentiment prediction\" for example, we don't care about getting information from the future, we want to allow every node to talk with every other nodes. so in the case we will drop the `masked_fill` operation \n",
    "\n",
    "In Decoder block - we don't want to allow future nodes to communicate with the preset or the past (because that will compromise the answer), so in that case we do use `masked_fill` operation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1541c3f97e17576"
  },
  {
   "cell_type": "markdown",
   "source": [
    "cross-attention - when we pull k and v information from a set of different nodes, using it for queries for current nodes\n",
    "\n",
    "self-attention - when we only use q,k,v for a \"closed\" group of nodes. \n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b45d649df31956f2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
